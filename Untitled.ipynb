{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17497740",
   "metadata": {},
   "source": [
    "Importing the necessary Libraries for the Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96caeacd-90f4-41e3-9534-5e62c56ac759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ee3417",
   "metadata": {},
   "source": [
    "Function for loading the Images from The datasets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd60f54d-6070-477e-84b7-daa8bb7ccbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(meta_data_path, image_folder):\n",
    "  \n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    for relation_folder in os.listdir(image_folder):\n",
    "        relation_path = os.path.join(image_folder, relation_folder)\n",
    "\n",
    "        mat_file = os.path.join(meta_data_path, f\"{relation_folder}.mat\")\n",
    "        data = loadmat(mat_file)['pairs']\n",
    "        for pair in data:\n",
    "                img1 = os.path.join(relation_path, str(pair[2][0]))\n",
    "                img2 = os.path.join(relation_path, str(pair[3][0]))\n",
    "                label = pair[1][0]  # 1 for similar, 0 for dissimilar\n",
    "                \n",
    "                pairs.append((img1, img2))\n",
    "                labels.append(label)\n",
    "                \n",
    "    return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f42c3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    image=cv.imread(img_path)\n",
    "    image_resized = cv.resize(image, (224, 224))\n",
    "    image_normalized = image_resized / 255.0 \n",
    "    image_transposed = np.transpose(image_normalized, (2, 0, 1)) #As pytorch prefers the image tensore in form of (Chnnel, Height, Width)\n",
    "    return torch.tensor(image_transposed, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "630cd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_split(pairs, labels):\n",
    "   \n",
    "    preprocessed_pairs = []\n",
    "    for img1_path, img2_path in pairs:\n",
    "\n",
    "            img1 = preprocess_image(img1_path)\n",
    "            img2 = preprocess_image(img2_path)\n",
    "            preprocessed_pairs.append((img1, img2))\n",
    "\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float32) \n",
    "    return preprocessed_pairs, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b8dd692-d7c2-4209-92c1-45b10eecf3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First dataset paths\n",
    "meta_data_path = r\"E:\\SNN project\\data\\KinFaceW-II\\meta_data\"\n",
    "image_folder = r\"E:\\SNN project\\data\\KinFaceW-II\\images\"\n",
    "\n",
    "# Load metadata and preprocess first dataset\n",
    "pairs_1, labels_1 = load_metadata(meta_data_path, image_folder)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_pairs_1, temp_pairs_1, train_labels_1, temp_labels_1 = train_test_split(\n",
    "    pairs_1, labels_1, test_size=0.3, random_state=42\n",
    ")\n",
    "val_pairs_1, test_pairs_1, val_labels_1, test_labels_1 = train_test_split(\n",
    "    temp_pairs_1, temp_labels_1, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess data\n",
    "train_pairs_preprocessed_1, train_labels_tensor_1 = preprocess_split(train_pairs_1, train_labels_1)\n",
    "val_pairs_preprocessed_1, val_labels_tensor_1 = preprocess_split(val_pairs_1, val_labels_1)\n",
    "test_pairs_preprocessed_1, test_labels_tensor_1 = preprocess_split(test_pairs_1, test_labels_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a87566d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second dataset paths\n",
    "meta_data_path_2 = r\"E:\\SNN project\\data\\KinFaceW-I\\meta_data\"\n",
    "image_folder_2 = r\"E:\\SNN project\\data\\KinFaceW-I\\images\"\n",
    "\n",
    "# Load metadata and preprocess second dataset\n",
    "pairs_2, labels_2 = load_metadata(meta_data_path_2, image_folder_2)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_pairs_2, temp_pairs_2, train_labels_2, temp_labels_2 = train_test_split(\n",
    "    pairs_2, labels_2, test_size=0.3, random_state=42\n",
    ")\n",
    "val_pairs_2, test_pairs_2, val_labels_2, test_labels_2 = train_test_split(\n",
    "    temp_pairs_2, temp_labels_2, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess data\n",
    "train_pairs_preprocessed_2, train_labels_tensor_2 = preprocess_split(train_pairs_2, train_labels_2)\n",
    "val_pairs_preprocessed_2, val_labels_tensor_2 = preprocess_split(val_pairs_2, val_labels_2)\n",
    "test_pairs_preprocessed_2, test_labels_tensor_2 = preprocess_split(test_pairs_2, test_labels_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bc7fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training Pairs: 2146\n",
      "Combined Validation Pairs: 460\n",
      "Combined Test Pairs: 460\n"
     ]
    }
   ],
   "source": [
    "# Combine training data\n",
    "train_pairs_combined = train_pairs_preprocessed_1 + train_pairs_preprocessed_2\n",
    "train_labels_combined = torch.cat((train_labels_tensor_1, train_labels_tensor_2), dim=0)\n",
    "\n",
    "# Combine validation data\n",
    "val_pairs_combined = val_pairs_preprocessed_1 + val_pairs_preprocessed_2\n",
    "val_labels_combined = torch.cat((val_labels_tensor_1, val_labels_tensor_2), dim=0)\n",
    "\n",
    "# Combine test data\n",
    "test_pairs_combined = test_pairs_preprocessed_1 + test_pairs_preprocessed_2\n",
    "test_labels_combined = torch.cat((test_labels_tensor_1, test_labels_tensor_2), dim=0)\n",
    "\n",
    "# Print dataset stats\n",
    "print(f\"Combined Training Pairs: {len(train_pairs_combined)}\")\n",
    "print(f\"Combined Validation Pairs: {len(val_pairs_combined)}\")\n",
    "print(f\"Combined Test Pairs: {len(test_pairs_combined)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f826763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Inherit_Dataset(Dataset):\n",
    "    def __init__(self, preprocessed_pairs, labels):\n",
    "        self.pairs = preprocessed_pairs\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1, img2 = self.pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return img1, img2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54b8f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Inherit_Dataset(train_pairs_combined, train_labels_combined)\n",
    "val_dataset = Inherit_Dataset(val_pairs_combined, val_labels_combined)\n",
    "test_dataset = Inherit_Dataset(test_pairs_combined, test_labels_combined)\n",
    "\n",
    "batch_size=16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e4dfc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN_ARCHITECTURE(nn.Module): \n",
    "  def __init__(self):  \n",
    "    super(SNN_ARCHITECTURE,self).__init__() \n",
    "    resnet = models.resnet101(pretrained=True)\n",
    "    self.cnn = nn.Sequential(*list(resnet.children())[:-1])  \n",
    "\n",
    "\n",
    "    self.fc = nn.Sequential(                           \n",
    "      #nn.Linear(64 * 56 * 56, 512),\n",
    "      nn.Linear(2048, 4096),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4096, 8192),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(8192, 512)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.cnn(x)\n",
    "    x=torch.flatten(x,1)\n",
    "    x = self.fc(x)\n",
    "    return x\n",
    "  \n",
    "  def Euclidean_distance(self, image_1, image_2):\n",
    "   \n",
    "    embed_1=self.forward(image_1)\n",
    "    embed_2=self.forward(image_2)\n",
    "    euclidean_distance = F.pairwise_distance(embed_1, embed_2)\n",
    "\n",
    "    return euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba17584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Fxn(nn.Module):\n",
    "    def __init__(self,margin=1.5):\n",
    "        super(Loss_Fxn, self).__init__()\n",
    "        self.margin=margin\n",
    "\n",
    "    def forward(self, euclidean_distance, label):\n",
    "        Contrastive_loss =0.5*label*(euclidean_distance)**2+(1-label)*torch.clamp(self.margin-euclidean_distance, min=0)**2\n",
    "        return torch.mean(Contrastive_loss) #Contrastive_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f88fa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ankur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ankur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SNN_ARCHITECTURE().to(device)\n",
    "criterion = Loss_Fxn(margin=1.5)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60fc501a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 0.9157 Validation Loss: 5.9196 Validation Accuracy: 0.5935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img1, img2, label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m----> 8\u001b[0m     img1, img2, label \u001b[38;5;241m=\u001b[39m \u001b[43mimg1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, img2\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m     distance \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mEuclidean_distance(img1, img2)\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(distance, label)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for img1, img2, label in train_loader:\n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "\n",
    "        distance = model.Euclidean_distance(img1, img2)\n",
    "\n",
    "        loss = criterion(distance, label)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    y_true,y_pred=[],[]\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, label in val_loader:\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "\n",
    "            distance = model.Euclidean_distance(img1, img2)\n",
    "            loss = criterion(distance, label)\n",
    "            val_loss += loss.item()\n",
    "            y_true.extend(label.cpu().numpy())\n",
    "            y_pred.extend((distance<0.5).cpu().numpy())\n",
    "\n",
    "    accuracy=accuracy_score(y_true,y_pred)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss / len(train_loader):.4f}\",f\"Validation Loss: {val_loss / len(val_loader):.4f}\",f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ed14a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m      2\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "test_accuracy = 0\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (img1, img2, label) in enumerate(test_loader):\n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        distance = model.Euclidean_distance(img1, img2)\n",
    "        y_true.extend(label.cpu().numpy())\n",
    "        y_pred.extend((distance < 0.5).cpu().numpy())\n",
    "\n",
    "        # Visualize a few samples\n",
    "        if idx < 5:  # Visualize the first 5 pairs\n",
    "            for i in range(min(len(img1), 5)):  # Visualize up to 5 pairs in the current batch\n",
    "                visualize_results(img1[i], img2[i], distance[i], label[i], threshold=0.5)\n",
    "\n",
    "# Compute overall test accuracy\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
